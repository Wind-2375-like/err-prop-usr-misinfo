{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')  # To ensure utils can be imported if needed\n",
    "\n",
    "from utils.data_loader import load_precomputed_results\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "import tiktoken # for token counting\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../pcd_data/finetune/correction_training_set.jsonl\"\n",
    "# data_path = \"../pcd_data/finetune/correction_training_set_100.jsonl\"\n",
    "\n",
    "# Load the dataset\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "        \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        \n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "    \n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 16385 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 16,385 token limit, they will be truncated during fine-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 16385\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
    "print(f\"By default, this will cost ~${n_epochs * n_billing_tokens_in_dataset / 1000000 * 3:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../api_key/config.json\", 'r') as f:\n",
    "    openai_api_key = json.load(f)[\"openai_api_key\"]\n",
    "    \n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(f'curl https://api.openai.com/v1/files -H \"Authorization: Bearer {openai_api_key}\" -F purpose=\"fine-tune\" -F file=\"@../pcd_data/finetune/correction_training_set.jsonl\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.fine_tuning.jobs.create(\n",
    "    training_file=..., # The ID of the uploaded file; see the previous cell\n",
    "    model=\"gpt-4o-mini-2024-07-18\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List 10 fine-tuning jobs\n",
    "client.fine_tuning.jobs.list(limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=..., # The model name is in the `fine_tuned_model` field in the previous cell.\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "First run `python run_experiments_finetune.py`.\n",
    "\n",
    "Then you will have `exp_results/mix/test_400_perturbed_premise_gpt-4o-mini-sft.jsonl` and execute the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load precomputed results\n",
    "df_results = load_precomputed_results(\"../exp_results/eval/results.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_loader import load_dataset_results, load_api_key\n",
    "from utils.evaluator.cot_evaluator import CoTEvaluator\n",
    "from utils.analysis.metrics import perturbation_ratio_given_correct\n",
    "from utils.analysis.bootstrapping import bootstrap, bootstrap_with_ratios\n",
    "from utils.analysis.data_processing import assign_confidence_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "datasets = [\"mix\"]\n",
    "models = [\n",
    "    \"gpt-4o-mini-sft\",\n",
    "]\n",
    "mode = \"premise\"\n",
    "sample_size = 400\n",
    "columns = [\"base_original\", \"base_misinformed\", \"inst_original\", \"inst_misinformed\", \"ft_original\", \"ft_misinformed\", \"inst_ft_original\", \"inst_ft_misinformed\"]\n",
    "\n",
    "with open(\"../api_key/config_yichen.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "    api_key = config[\"openai_api_key\"]\n",
    "    \n",
    "cot_evaluator = CoTEvaluator(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating CoT outputs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Models:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Models: 100%|██████████| 1/1 [00:01<00:00,  1.63s/it]\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# Step 1: Evaluate CoT and Store Results\n",
    "############################################\n",
    "overall_results = {}\n",
    "print(\"Evaluating CoT outputs...\")\n",
    "for model_name in tqdm(models, desc=\"Models\"):\n",
    "    overall_results[model_name] = {}\n",
    "    for dataset_name in tqdm(datasets, desc=f\"{model_name} Datasets\", leave=False):\n",
    "        try:\n",
    "            df_sample = pd.read_json(f\"../exp_results/mix/test_{sample_size}_perturbed_{mode}_{model_name}.jsonl\", lines=True)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        df_sample[\"correct_answer\"] = df_sample[\"correct_answer\"].astype(str)\n",
    "\n",
    "        df_eval_results = pd.DataFrame(columns=columns)\n",
    "        for idx, row in tqdm(df_sample.iterrows(), total=df_sample.shape[0], desc=f\"Evaluating {model_name}-{dataset_name}\", leave=False):\n",
    "            evaluation_results = {}\n",
    "            for column in columns:\n",
    "                if \"original\" in column or column == \"prfx_q\":\n",
    "                    evaluation_results[column] = cot_evaluator.evaluate_cot_list(\n",
    "                        row[column], row[\"correct_answer\"]\n",
    "                    )\n",
    "                else:\n",
    "                    evaluation_results[column] = {}\n",
    "                    for role in [\"human\"]:\n",
    "                        if role in row[column]:\n",
    "                            evaluation_results[column][role] = cot_evaluator.evaluate_cot_list(\n",
    "                                row[column][role],\n",
    "                                row[\"correct_answer\"]\n",
    "                            )\n",
    "                        else:\n",
    "                            evaluation_results[column][role] = {}\n",
    "            df_eval_results = pd.concat([df_eval_results, pd.DataFrame([evaluation_results], index=[idx])])\n",
    "\n",
    "        df_eval_results.reset_index(drop=True, inplace=True)\n",
    "        overall_results[model_name][dataset_name] = df_eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing bootstrapping results for radar/table...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Radar Bootstrapping - Models:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "Radar Bootstrapping - Models: 100%|██████████| 1/1 [04:29<00:00, 269.69s/it]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Computing bootstrapping results for radar/table...\")\n",
    "for model_name in tqdm(models, desc=\"Radar Bootstrapping - Models\"):\n",
    "    if model_name not in overall_results:\n",
    "        continue\n",
    "    df_results[model_name] = {}\n",
    "    for source in tqdm([\"human\"], desc=f\"{model_name} Sources\", leave=False):\n",
    "        mean_df = pd.DataFrame(index=datasets, columns=columns)\n",
    "        margin_df = pd.DataFrame(index=datasets, columns=columns)\n",
    "        ratio_lists = {col: {} for col in columns}\n",
    "\n",
    "        for column in tqdm(columns, desc=f\"{model_name}-{source} Columns\", leave=False):\n",
    "            for dataset_name in tqdm(datasets, desc=f\"{model_name}-{source}-{column} Datasets\", leave=False):\n",
    "                if dataset_name not in overall_results[model_name]:\n",
    "                    mean_df.loc[dataset_name, column] = np.nan\n",
    "                    margin_df.loc[dataset_name, column] = (np.nan, np.nan)\n",
    "                    ratio_lists[column][dataset_name] = []\n",
    "                    continue\n",
    "\n",
    "                df_data = overall_results[model_name][dataset_name].reset_index(drop=True)\n",
    "                if \"original\" in column:\n",
    "                    original_column = \"base_original\"\n",
    "                else:\n",
    "                    original_column = \"prfx_q\"\n",
    "                ratios = bootstrap_with_ratios(\n",
    "                    df_data,\n",
    "                    lambda x: perturbation_ratio_given_correct(\n",
    "                        x,\n",
    "                        cij=original_column,\n",
    "                        pij=column,\n",
    "                        evaluation_type=\"overall_correct\",\n",
    "                        perturbation_role=source if column != original_column else None\n",
    "                    ),\n",
    "                    n=1000,\n",
    "                )\n",
    "                mean_val = np.mean(ratios)\n",
    "                lb_val = np.percentile(ratios, 2.5)\n",
    "                ub_val = np.percentile(ratios, 97.5)\n",
    "\n",
    "                mean_df.loc[dataset_name, column] = mean_val\n",
    "                margin_df.loc[dataset_name, column] = (lb_val, ub_val)\n",
    "                ratio_lists[column][dataset_name] = ratios\n",
    "\n",
    "        df_results[model_name][source] = {\n",
    "            \"mean\": mean_df,\n",
    "            \"margin\": margin_df,\n",
    "            \"ratio_lists\": ratio_lists\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../exp_results/eval/results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from utils.evaluator.cot_evaluator import CoTEvaluator\n",
    "import argparse\n",
    "from utils.data_loader import load_dataset_results, load_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_cot_evaluator(openai_api_key):\n",
    "    cot_evaluator = CoTEvaluator(api_key=openai_api_key, point_out_model_name='gpt-4o')\n",
    "    return cot_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_sample(args, openai_api_key):\n",
    "    # Instead of starting with a DataFrame, we'll start with an empty list\n",
    "    rows_to_add = []\n",
    "    cot_evaluator = initialize_cot_evaluator(openai_api_key)\n",
    "\n",
    "    for model_name in tqdm(args.model_names, desc=\"Loading samples\"):\n",
    "        # Load the dataset results (make sure load_dataset_results handles caching or is fast)\n",
    "        df_sample = pd.read_json(f\"../exp_results/mix/test_{args.sample_size}_perturbed_{args.mode}_{args.model_names[0]}.jsonl\", lines=True)\n",
    "        df_sample[\"correct_answer\"] = df_sample[\"correct_answer\"].astype(str)\n",
    "        # Using itertuples or iterrows can be considered; itertuples is slightly faster\n",
    "        for idx, row in tqdm(df_sample.iterrows(), total=len(df_sample), desc=f\"Processing {model_name}\"):\n",
    "            for source in [\"human\"]:\n",
    "                for i in range(5):\n",
    "                    try:\n",
    "                        original_output = row[\"base_original\"][f\"c_{i}\"]\n",
    "                        output = row[\"ft_misinformed\"][\"human\" if source == \"human\" else \"self\"][f\"c_{i}\"]\n",
    "                        try:\n",
    "                            overall_correct_original = cot_evaluator.answer_verifier(original_output[-1], row[\"correct_answer\"])\n",
    "                        except:\n",
    "                            continue\n",
    "                        # Append to rows_to_add if the original answer is correct\n",
    "                        if overall_correct_original:\n",
    "                            try:\n",
    "                                overall_correct = cot_evaluator.answer_verifier(output[-1], row[\"correct_answer\"])\n",
    "                                rows_to_add.append({\n",
    "                                    \"question\": row[\"question\"],\n",
    "                                    \"correct_answer\": row[\"correct_answer\"],\n",
    "                                    \"premise\": row[\"premise\"][source],\n",
    "                                    \"perturbed_premise\": row[\"perturbed_premise\"][source],\n",
    "                                    \"overall_correct\": overall_correct,\n",
    "                                    \"output\": output,\n",
    "                                    \"model\": model_name,\n",
    "                                    \"dataset\": args.dataset_name,\n",
    "                                })\n",
    "                            except:\n",
    "                                continue\n",
    "                    except KeyError as e:\n",
    "                        raise\n",
    "\n",
    "    # Convert the accumulated rows into a DataFrame once\n",
    "    df_aggregated = pd.DataFrame(rows_to_add)\n",
    "    \n",
    "    def filter_out_truncated_answer(row):\n",
    "        return 'answer' in row['output'][-1]\n",
    "\n",
    "    # Filter and sample\n",
    "    df_aggregated = df_aggregated[df_aggregated[\"output\"].apply(lambda x: x != [])]\n",
    "    df_aggregated = df_aggregated[df_aggregated.apply(filter_out_truncated_answer, axis=1)]\n",
    "\n",
    "    # Sample 400 from each column\n",
    "    df_sample = df_aggregated.sample(n=args.sample_size, random_state=42)\n",
    "    return df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_evaluation(df_sample, cot_evaluator, args):\n",
    "    df_evaluated = df_sample.copy()\n",
    "    df_evaluated[\"detection\"] = df_evaluated.apply(lambda x: None, axis=1)\n",
    "    df_evaluated[\"correction\"] = df_evaluated.apply(lambda x: None, axis=1)\n",
    "    df_evaluated[\"perturbation\"] = df_evaluated.apply(lambda x: None, axis=1)\n",
    "\n",
    "    # Use a tqdm progress bar\n",
    "    with tqdm(total=len(df_sample), desc=\"Evaluating samples\") as pbar:\n",
    "        for idx, row in tqdm(df_sample.iterrows(), total=len(df_sample)):\n",
    "            detection_label, detection_explanations = cot_evaluator.overall_point_out_error_verifier(row[\"question\"], row[\"output\"], row[\"perturbed_premise\"])\n",
    "            if detection_label:\n",
    "                detection_positions = cot_evaluator.point_out_position_verifier(row[\"question\"], row[\"output\"], row[\"perturbed_premise\"])\n",
    "                correction_label, correction_explanations = cot_evaluator.overall_success_correction_verifier(row[\"question\"], row[\"output\"], row[\"premise\"], row[\"perturbed_premise\"])\n",
    "            else:\n",
    "                detection_positions = []\n",
    "                correction_label, correction_explanations = False, detection_explanations\n",
    "            perturbation_label, perturbation_explanations = cot_evaluator.overall_perturbation_verifier(row[\"question\"], row[\"output\"], row[\"premise\"], row[\"perturbed_premise\"])\n",
    "                \n",
    "            df_evaluated.at[idx, \"detection\"] = {\"label\": detection_label, \"explanations\": detection_explanations, \"steps\": detection_positions}\n",
    "            df_evaluated.at[idx, \"correction\"] = {\"label\": correction_label, \"explanations\": correction_explanations, \"steps\": detection_positions}\n",
    "            df_evaluated.at[idx, \"perturbation\"] = {\"label\": perturbation_label, \"explanations\": perturbation_explanations}\n",
    "            \n",
    "            # Get usage from cot_evaluator and update tqdm\n",
    "            try:\n",
    "                usage = cot_evaluator.get_usage()[\"gpt-4o\"]\n",
    "            except:\n",
    "                usage = {\"prompt_tokens\": 0, \"completion_tokens\": 0, \"total_tokens\": 0}\n",
    "            pbar.set_postfix({\n",
    "                \"Prompt tokens\": usage['prompt_tokens'], \n",
    "                \"Completion tokens\": usage['completion_tokens'], \n",
    "                \"Total tokens\": usage['total_tokens']\n",
    "            })\n",
    "            pbar.update(1)\n",
    "            \n",
    "    return df_evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing gpt-4o-mini-sft: 100%|██████████| 400/400 [00:00<00:00, 1682.27it/s]\n",
      "Loading samples: 100%|██████████| 1/1 [00:00<00:00,  2.28it/s]\n",
      "100%|██████████| 400/400 [48:59<00:00,  7.35s/it]59<00:00,  6.94s/it, Prompt tokens=3164072, Completion tokens=75951, Total tokens=3240023]\n",
      "Evaluating samples: 100%|██████████| 400/400 [48:59<00:00,  7.35s/it, Prompt tokens=3164072, Completion tokens=75951, Total tokens=3240023]\n"
     ]
    }
   ],
   "source": [
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.sample_size = 400\n",
    "        self.api_config_file_path = \"../api_key/config_yiyang.json\"\n",
    "        self.dataset_name = \"mix\"\n",
    "        self.model_names = [\"gpt-4o-mini-sft\"]\n",
    "        self.mode = \"premise\"\n",
    "        self.columns = [\"base_original\", \"base_misinformed\", \"inst_original\", \"inst_misinformed\", \"ft_original\", \"ft_misinformed\", \"inst_ft_original\", \"inst_ft_misinformed\"]\n",
    "        \n",
    "args = Args()\n",
    "openai_api_key = load_api_key(args.api_config_file_path)\n",
    "cot_evaluator = initialize_cot_evaluator(openai_api_key)\n",
    "df_sample = aggregate_sample(args, openai_api_key)\n",
    "df_evaluated = perform_evaluation(df_sample, cot_evaluator, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>premise</th>\n",
       "      <th>perturbed_premise</th>\n",
       "      <th>overall_correct</th>\n",
       "      <th>output</th>\n",
       "      <th>model</th>\n",
       "      <th>dataset</th>\n",
       "      <th>detection</th>\n",
       "      <th>correction</th>\n",
       "      <th>perturbation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1706</th>\n",
       "      <td>If $n$ is 1 less than a multiple of 50, what i...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>$n = 50k - 1$; $n^2 + 2n + 3 = (50k - 1)^2 + 2...</td>\n",
       "      <td>$k = 40 + n - 1$; $k^2 \\div 2k - 3 = (40n - 1)...</td>\n",
       "      <td>True</td>\n",
       "      <td>[The first formula from the user contains a mi...</td>\n",
       "      <td>gpt-4o-mini-sft</td>\n",
       "      <td>mix</td>\n",
       "      <td>{'label': True, 'explanations': 'The model's s...</td>\n",
       "      <td>{'label': True, 'explanations': 'The model cor...</td>\n",
       "      <td>{'label': False, 'explanations': 'Step 1 detec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>a bullet train 150 m long is running with a sp...</td>\n",
       "      <td>15 sec</td>\n",
       "      <td>$T = \\frac{D}{S_{\\text{relative}}}$, where $S_...</td>\n",
       "      <td>$T = \\frac{S_{\\text{relative}}}{D}$, where $S_...</td>\n",
       "      <td>True</td>\n",
       "      <td>[The first formula from the user contains a mi...</td>\n",
       "      <td>gpt-4o-mini-sft</td>\n",
       "      <td>mix</td>\n",
       "      <td>{'label': True, 'explanations': 'The model cor...</td>\n",
       "      <td>{'label': True, 'explanations': 'The model cor...</td>\n",
       "      <td>{'label': False, 'explanations': 'Step 1 corre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1873</th>\n",
       "      <td>The sum of the $x$-coordinates of the vertices...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>$\\text{Sum of x-coordinates of vertices} = X$;...</td>\n",
       "      <td>$\\text{Sum of x-coordinates of midpoints} = X$...</td>\n",
       "      <td>True</td>\n",
       "      <td>[The first formula from the user contains a mi...</td>\n",
       "      <td>gpt-4o-mini-sft</td>\n",
       "      <td>mix</td>\n",
       "      <td>{'label': True, 'explanations': 'The model's s...</td>\n",
       "      <td>{'label': False, 'explanations': 'The model's ...</td>\n",
       "      <td>{'label': True, 'explanations': 'Step 7 is cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>Ruby is 6 times older than Sam. In 9 years, Ru...</td>\n",
       "      <td>6</td>\n",
       "      <td>$\\text{Ruby's age} = 6 \\times \\text{Sam's age}...</td>\n",
       "      <td>$\\text{Sam's age} = 6 \\times \\text{Ruby's age}...</td>\n",
       "      <td>True</td>\n",
       "      <td>[The first formula from the user contains a mi...</td>\n",
       "      <td>gpt-4o-mini-sft</td>\n",
       "      <td>mix</td>\n",
       "      <td>{'label': True, 'explanations': 'The model cor...</td>\n",
       "      <td>{'label': True, 'explanations': 'The model cor...</td>\n",
       "      <td>{'label': False, 'explanations': 'Step 1 detec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>Antoine's french onion soup recipe calls for 2...</td>\n",
       "      <td>2</td>\n",
       "      <td>$\\text{Onion cost} = 2 \\times \\text{Onion weig...</td>\n",
       "      <td>$\\text{Onion cost} = 3 \\div \\text{Onion weight...</td>\n",
       "      <td>True</td>\n",
       "      <td>[The first formula from the user contains a mi...</td>\n",
       "      <td>gpt-4o-mini-sft</td>\n",
       "      <td>mix</td>\n",
       "      <td>{'label': True, 'explanations': 'The model's s...</td>\n",
       "      <td>{'label': True, 'explanations': 'The model cor...</td>\n",
       "      <td>{'label': False, 'explanations': 'Step 1 detec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>the circumferences of the fore and hind - whee...</td>\n",
       "      <td>24 metres</td>\n",
       "      <td>$\\text{Distance} = \\text{lcm}\\left(\\text{Circu...</td>\n",
       "      <td>$\\text{Distance} = \\text{lcm}\\left(\\text{Circu...</td>\n",
       "      <td>True</td>\n",
       "      <td>[The given formula from the user contains a mi...</td>\n",
       "      <td>gpt-4o-mini-sft</td>\n",
       "      <td>mix</td>\n",
       "      <td>{'label': True, 'explanations': 'The model cor...</td>\n",
       "      <td>{'label': True, 'explanations': 'The model cor...</td>\n",
       "      <td>{'label': False, 'explanations': 'Step 1 corre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>The expression $3x^2 + 14x + 8$ can be written...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>$3x^2 + 14x + 8 = (3x + A)(x + B)$; $A = \\frac...</td>\n",
       "      <td>$3x^2 \\div 15x \\div 8 = (3 + A)(B \\div x)$; $A...</td>\n",
       "      <td>True</td>\n",
       "      <td>[The first formula from the user contains a mi...</td>\n",
       "      <td>gpt-4o-mini-sft</td>\n",
       "      <td>mix</td>\n",
       "      <td>{'label': True, 'explanations': 'The model's s...</td>\n",
       "      <td>{'label': False, 'explanations': 'The model's ...</td>\n",
       "      <td>{'label': False, 'explanations': 'Step 1 detec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1821</th>\n",
       "      <td>Find four-thirds of $\\frac{9}{2}$.</td>\n",
       "      <td>6.0</td>\n",
       "      <td>$\\frac{4}{3} \\times \\frac{9}{2}$</td>\n",
       "      <td>$4 + 30 \\times \\frac{9}{1}$</td>\n",
       "      <td>True</td>\n",
       "      <td>[The given formula from the user contains a mi...</td>\n",
       "      <td>gpt-4o-mini-sft</td>\n",
       "      <td>mix</td>\n",
       "      <td>{'label': True, 'explanations': 'The first ste...</td>\n",
       "      <td>{'label': True, 'explanations': 'The model cor...</td>\n",
       "      <td>{'label': False, 'explanations': 'Step 1 corre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>The equation of the circle that passes through...</td>\n",
       "      <td>-120.0</td>\n",
       "      <td>$(x-h)^2+(y-k)^2=r^2$; $(x-2)^2+(y-3)^2=(-1-2)...</td>\n",
       "      <td>$(y-k)^2 \\times (x-h)^2 = r^2$; $(y-2)^2 \\time...</td>\n",
       "      <td>True</td>\n",
       "      <td>[The first formula from the user contains a mi...</td>\n",
       "      <td>gpt-4o-mini-sft</td>\n",
       "      <td>mix</td>\n",
       "      <td>{'label': True, 'explanations': 'The model's s...</td>\n",
       "      <td>{'label': True, 'explanations': 'The model cor...</td>\n",
       "      <td>{'label': False, 'explanations': 'Step 1 detec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>m and n are the x and y coordinates , respecti...</td>\n",
       "      <td>3</td>\n",
       "      <td>$m = \\frac{n}{4} - \\frac{2}{5}$; $m + p = \\fra...</td>\n",
       "      <td>$n = \\frac{m}{4} \\cdot \\frac{3}{6}$; $p + m = ...</td>\n",
       "      <td>True</td>\n",
       "      <td>[The first formula from the user contains a mi...</td>\n",
       "      <td>gpt-4o-mini-sft</td>\n",
       "      <td>mix</td>\n",
       "      <td>{'label': True, 'explanations': 'The model's s...</td>\n",
       "      <td>{'label': False, 'explanations': 'The model's ...</td>\n",
       "      <td>{'label': False, 'explanations': 'Step 1 detec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question correct_answer  \\\n",
       "1706  If $n$ is 1 less than a multiple of 50, what i...            2.0   \n",
       "1199  a bullet train 150 m long is running with a sp...         15 sec   \n",
       "1873  The sum of the $x$-coordinates of the vertices...           10.0   \n",
       "276   Ruby is 6 times older than Sam. In 9 years, Ru...              6   \n",
       "417   Antoine's french onion soup recipe calls for 2...              2   \n",
       "...                                                 ...            ...   \n",
       "1341  the circumferences of the fore and hind - whee...      24 metres   \n",
       "676   The expression $3x^2 + 14x + 8$ can be written...           -2.0   \n",
       "1821                 Find four-thirds of $\\frac{9}{2}$.            6.0   \n",
       "614   The equation of the circle that passes through...         -120.0   \n",
       "1343  m and n are the x and y coordinates , respecti...              3   \n",
       "\n",
       "                                                premise  \\\n",
       "1706  $n = 50k - 1$; $n^2 + 2n + 3 = (50k - 1)^2 + 2...   \n",
       "1199  $T = \\frac{D}{S_{\\text{relative}}}$, where $S_...   \n",
       "1873  $\\text{Sum of x-coordinates of vertices} = X$;...   \n",
       "276   $\\text{Ruby's age} = 6 \\times \\text{Sam's age}...   \n",
       "417   $\\text{Onion cost} = 2 \\times \\text{Onion weig...   \n",
       "...                                                 ...   \n",
       "1341  $\\text{Distance} = \\text{lcm}\\left(\\text{Circu...   \n",
       "676   $3x^2 + 14x + 8 = (3x + A)(x + B)$; $A = \\frac...   \n",
       "1821                   $\\frac{4}{3} \\times \\frac{9}{2}$   \n",
       "614   $(x-h)^2+(y-k)^2=r^2$; $(x-2)^2+(y-3)^2=(-1-2)...   \n",
       "1343  $m = \\frac{n}{4} - \\frac{2}{5}$; $m + p = \\fra...   \n",
       "\n",
       "                                      perturbed_premise  overall_correct  \\\n",
       "1706  $k = 40 + n - 1$; $k^2 \\div 2k - 3 = (40n - 1)...             True   \n",
       "1199  $T = \\frac{S_{\\text{relative}}}{D}$, where $S_...             True   \n",
       "1873  $\\text{Sum of x-coordinates of midpoints} = X$...             True   \n",
       "276   $\\text{Sam's age} = 6 \\times \\text{Ruby's age}...             True   \n",
       "417   $\\text{Onion cost} = 3 \\div \\text{Onion weight...             True   \n",
       "...                                                 ...              ...   \n",
       "1341  $\\text{Distance} = \\text{lcm}\\left(\\text{Circu...             True   \n",
       "676   $3x^2 \\div 15x \\div 8 = (3 + A)(B \\div x)$; $A...             True   \n",
       "1821                        $4 + 30 \\times \\frac{9}{1}$             True   \n",
       "614   $(y-k)^2 \\times (x-h)^2 = r^2$; $(y-2)^2 \\time...             True   \n",
       "1343  $n = \\frac{m}{4} \\cdot \\frac{3}{6}$; $p + m = ...             True   \n",
       "\n",
       "                                                 output            model  \\\n",
       "1706  [The first formula from the user contains a mi...  gpt-4o-mini-sft   \n",
       "1199  [The first formula from the user contains a mi...  gpt-4o-mini-sft   \n",
       "1873  [The first formula from the user contains a mi...  gpt-4o-mini-sft   \n",
       "276   [The first formula from the user contains a mi...  gpt-4o-mini-sft   \n",
       "417   [The first formula from the user contains a mi...  gpt-4o-mini-sft   \n",
       "...                                                 ...              ...   \n",
       "1341  [The given formula from the user contains a mi...  gpt-4o-mini-sft   \n",
       "676   [The first formula from the user contains a mi...  gpt-4o-mini-sft   \n",
       "1821  [The given formula from the user contains a mi...  gpt-4o-mini-sft   \n",
       "614   [The first formula from the user contains a mi...  gpt-4o-mini-sft   \n",
       "1343  [The first formula from the user contains a mi...  gpt-4o-mini-sft   \n",
       "\n",
       "     dataset                                          detection  \\\n",
       "1706     mix  {'label': True, 'explanations': 'The model's s...   \n",
       "1199     mix  {'label': True, 'explanations': 'The model cor...   \n",
       "1873     mix  {'label': True, 'explanations': 'The model's s...   \n",
       "276      mix  {'label': True, 'explanations': 'The model cor...   \n",
       "417      mix  {'label': True, 'explanations': 'The model's s...   \n",
       "...      ...                                                ...   \n",
       "1341     mix  {'label': True, 'explanations': 'The model cor...   \n",
       "676      mix  {'label': True, 'explanations': 'The model's s...   \n",
       "1821     mix  {'label': True, 'explanations': 'The first ste...   \n",
       "614      mix  {'label': True, 'explanations': 'The model's s...   \n",
       "1343     mix  {'label': True, 'explanations': 'The model's s...   \n",
       "\n",
       "                                             correction  \\\n",
       "1706  {'label': True, 'explanations': 'The model cor...   \n",
       "1199  {'label': True, 'explanations': 'The model cor...   \n",
       "1873  {'label': False, 'explanations': 'The model's ...   \n",
       "276   {'label': True, 'explanations': 'The model cor...   \n",
       "417   {'label': True, 'explanations': 'The model cor...   \n",
       "...                                                 ...   \n",
       "1341  {'label': True, 'explanations': 'The model cor...   \n",
       "676   {'label': False, 'explanations': 'The model's ...   \n",
       "1821  {'label': True, 'explanations': 'The model cor...   \n",
       "614   {'label': True, 'explanations': 'The model cor...   \n",
       "1343  {'label': False, 'explanations': 'The model's ...   \n",
       "\n",
       "                                           perturbation  \n",
       "1706  {'label': False, 'explanations': 'Step 1 detec...  \n",
       "1199  {'label': False, 'explanations': 'Step 1 corre...  \n",
       "1873  {'label': True, 'explanations': 'Step 7 is cor...  \n",
       "276   {'label': False, 'explanations': 'Step 1 detec...  \n",
       "417   {'label': False, 'explanations': 'Step 1 detec...  \n",
       "...                                                 ...  \n",
       "1341  {'label': False, 'explanations': 'Step 1 corre...  \n",
       "676   {'label': False, 'explanations': 'Step 1 detec...  \n",
       "1821  {'label': False, 'explanations': 'Step 1 corre...  \n",
       "614   {'label': False, 'explanations': 'Step 1 detec...  \n",
       "1343  {'label': False, 'explanations': 'Step 1 detec...  \n",
       "\n",
       "[400 rows x 11 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evaluated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Sankey Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plotters.sankey_plot import plot_sankey_data, plot_position_distribution\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluated.to_json(\"../exp_results/mix/test_400_premise_evaluated_gpt-4o-mini-sft.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.02% of the successful corrections have a correct overall answer.\n",
      "42.31% of the overall inaccuracies have a failed or No Identification.\n"
     ]
    }
   ],
   "source": [
    "# Ensure 'overall_correct_original', 'overall_correct' and 'point_out_error' are booleans\n",
    "df_evaluated['overall_correct'] = df_evaluated['overall_correct'].astype(bool)\n",
    "df_evaluated['detection'] = df_evaluated['detection'].apply(lambda x: x['label'])\n",
    "df_evaluated['correction'] = df_evaluated['correction'].apply(lambda x: x['label'])\n",
    "\n",
    "df_evaluated['Detect'] = df_evaluated['detection'].map({True: 'Corr\\\\n', False: 'N-Corr    \\\\n'})\n",
    "\n",
    "# Three-state mapping for 'Correct' and 'CorrectPrompt'\n",
    "def map_correct(row):\n",
    "    if not row['detection']:\n",
    "        return 'N-Corr\\\\n'\n",
    "    elif row['detection'] and not row['correction']:\n",
    "        return 'NF-Corr\\\\n'\n",
    "    elif row['detection'] and row['correction']:\n",
    "        return 'F-Corr\\\\n'\n",
    "    \n",
    "df_evaluated['Correct'] = df_evaluated.apply(map_correct, axis=1)\n",
    "df_evaluated['Overall'] = df_evaluated['overall_correct'].map({True: '✅ Answer\\\\n', False: '❎ Answer\\\\n'})\n",
    "\n",
    "# Group by 'Original', 'Overall' and 'ErrorPoint' to get counts\n",
    "grouped = df_evaluated.groupby(['Detect', 'Correct', 'Overall']).size().reset_index(name='counts')\n",
    "html_template = plot_sankey_data(['Detect', 'Correct', 'Overall'], grouped)\n",
    "\n",
    "ratio = grouped[(grouped['Correct'] == 'F-Corr\\\\n') & (grouped['Overall'] == '✅ Answer\\\\n')].counts.sum() / grouped[grouped['Correct'] == 'F-Corr\\\\n'].counts.sum()\n",
    "print(f\"{ratio*100:.2f}% of the successful corrections have a correct overall answer.\")\n",
    "ratio = 1 - grouped[(grouped['Correct'] == 'F-Corr\\\\n') & (grouped['Overall'] == '❎ Answer\\\\n')].counts.sum() / grouped[grouped['Overall'] == '❎ Answer\\\\n'].counts.sum()\n",
    "print(f\"{ratio*100:.2f}% of the overall inaccuracies have a failed or No Identification.\")\n",
    "\n",
    "# Save the html file. I have no idea to save it as a pdf.\n",
    "with open(f\"../figures/sankey_finetune_gpt4o.html\", \"w\") as html_file:\n",
    "    html_file.write(html_template)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
